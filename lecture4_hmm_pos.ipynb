{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "PoS_dict = {\"the\":\"determiner\", \"can\": \"modal\", \"fly\":\"verb\"}\n",
    "\n",
    "tag_set = ['.', 'NUM', 'VERB', 'DET', 'ADV', 'CONJ', 'PRT', 'PRON', 'ADP', 'X', 'NOUN', 'ADJ']\n",
    "corpus = nltk.corpus.brown.tagged_sents(tagset='brown')\n",
    "\n",
    "unigram_tag_counts = Counter()\n",
    "bigram_tag_counts = Counter()\n",
    "word_tag_counts = Counter()\n",
    "word_tag_dict = {}\n",
    "for sent in corpus:\n",
    "    tokens, tags = zip(*sent)\n",
    "    padded_tags = [None]+list(tags)+[None]\n",
    "    for index,tag in enumerate(padded_tags):\n",
    "        unigram_tag_counts[tag]+=1\n",
    "        if index > 0:\n",
    "            bigram_tag_counts[(padded_tags[index-1],tag)] +=1\n",
    "            if index< len(padded_tags)-1:\n",
    "                word_tag_counts[(tag,tokens[index-1])] += 1\n",
    "                if tokens[index-1] not in word_tag_dict:\n",
    "                    word_tag_dict[tokens[index-1]] = [tag]\n",
    "                else:\n",
    "                    if tag not in word_tag_dict[tokens[index-1]]:\n",
    "                        word_tag_dict[tokens[index-1]].append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def bigram_LM(tags_y):\n",
    "    padded_tags = [None]+list(tags_y)+[None]\n",
    "    tag_bigrams = []\n",
    "    for index in range(len(padded_tags)-1):\n",
    "        tag_bigrams.append((padded_tags[index],padded_tags[index+1]))\n",
    "    print(tag_bigrams)\n",
    "    prob_x = 1.0\n",
    "    for bg in tag_bigrams:\n",
    "        if bg[0] == None:\n",
    "            prob_bg = (bigram_tag_counts[bg])/(len(corpus))\n",
    "        else:\n",
    "            prob_bg = (bigram_tag_counts[bg])/(unigram_tag_counts[bg[0]])\n",
    "        prob_x = prob_x *prob_bg\n",
    "        print(str(bg)+\":\"+str(prob_bg))\n",
    "    return prob_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Part of speech tagging<br> with hidden Markov models</h2>\n",
    "<p style=\"text-align:center\">\n",
    "Natural Language Processing<br>\n",
    "(COM4513/6513)<br>\n",
    "<br>\n",
    "<a href=\"http://andreasvlachos.github.io\">Andreas Vlachos</a><br>\n",
    "a.vlachos@sheffield.ac.uk<br>\n",
    "<small>Department of Computer Science<br>\n",
    "University of Sheffield\n",
    "</small>\n",
    "</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parts of speech (PoS)\n",
    "\n",
    "Word labels according to their syntactic function in a sentence:\n",
    "\n",
    "| The       | results  | appear | in          | today| 's         |news |\n",
    "| --------- |----------| -------| ------------|----- |------------|-----|\n",
    "| determiner       | noun     | verb   | preposition | noun | possesssive| noun|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What could they be useful for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- language modelling\n",
    "- syntactic parsing\n",
    "- named entity recognition\n",
    "- question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kinds of PoS tags\n",
    "\n",
    "Open class:\n",
    "- nouns\n",
    "    - proper nouns\n",
    "    - common nouns\n",
    "- verbs\n",
    "- adjectives\n",
    "- adverbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Closed class: determiners, prepositions, conjunctions, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PoS definitions\n",
    "\n",
    "Most research uses the [Penn Treebank PoS tag set](https://www.clips.uantwerpen.be/pages/mbsp-tags).\n",
    "\n",
    "Includes 45 tags making distinctions between\n",
    "  - verbs in active vs past tense\n",
    "  - nouns in singular vs plural number\n",
    "  - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most distinction are inspired by English. Recent work has focused on the [Universal PoS tag set](http://universaldependencies.org/u/pos/):\n",
    "- 17 coarse tags: one noun, one verb, etc.\n",
    "- developed considering 22 languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem setup\n",
    "\n",
    "Training data is word sequences with label sequences:\n",
    "\n",
    "\\begin{align}\n",
    "D_{train} & = \\{(\\mathbf{x}^1,\\mathbf{y}^1)...(\\mathbf{x}^M,\\mathbf{y}^M)\\} \\\\\n",
    "\\mathbf{x}^m & = [x_1,... x_N]\\\\\n",
    "\\mathbf{y}^m & = [y_1,... y_N]\n",
    "\\end{align}\n",
    "\n",
    "for example:\n",
    "\\begin{align}\n",
    "(\\mathbf{x},\\mathbf{y})=(&[I,studied,in,Sheffield],\\\\\n",
    "&[Pronoun,Verb,Preposition,ProperNoun])\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Learn a model that predicts the best label sequence:\n",
    "\\begin{align}\n",
    "\\mathbf{\\hat y} = \\mathop{\\arg \\max}\\limits_{\\mathbf{y} \\in \\cal Y^N} score(\\mathbf{x},\\mathbf{y})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Could we use a dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 'determiner', 'can': 'modal', 'fly': 'verb'}\n"
     ]
    }
   ],
   "source": [
    "print(PoS_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, but the same word can have different tags in different contexts.\n",
    "\n",
    "I | can | fly\n",
    "---|----|----\n",
    "pronoun | modal | verb\n",
    "\n",
    "vs:\n",
    "\n",
    "\n",
    "I | can | fly\n",
    "---|----|----\n",
    "pronoun | verb | noun\n",
    "\n",
    "*can* and 11.5% of the words in the Brown corpus have more than one tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Can we use a Markov model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our training data should be able to tell us that tagging\n",
    "\n",
    "I | can | fly\n",
    "---|----|----\n",
    "pronoun | modal | noun\n",
    "\n",
    "is unlikely. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Replace words $\\mathbf{x}$ with tags $\\mathbf{y}$:\n",
    "\n",
    "$$P(\\mathbf{y}) = \\prod_{n=1}^N P(y_n| y_{n-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Can we use a Markov model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 'PPSS'), ('PPSS', 'MD'), ('MD', 'NN'), ('NN', None)]\n",
      "(None, 'PPSS'):0.05486571328915242\n",
      "('PPSS', 'MD'):0.15229676858426314\n",
      "('MD', 'NN'):0.0017697691255731639\n",
      "('NN', None):0.00044598937495900833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.595274031841351e-09"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_LM(['PPSS','MD','NN']) # PPSS is the personal pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 'PPSS'), ('PPSS', 'VB'), ('VB', 'NN'), ('NN', None)]\n",
      "(None, 'PPSS'):0.05486571328915242\n",
      "('PPSS', 'VB'):0.23061875090566586\n",
      "('VB', 'NN'):0.04342148220698661\n",
      "('NN', None):0.00044598937495900833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.450331267007358e-07"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_LM(['PPSS','VB','NN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about the words?\n",
    "\n",
    "What we have is:\n",
    "\n",
    "$$P(\\mathbf{y}) = \\prod_{n=1}^N P(y_n| y_{n-1})$$\n",
    "\n",
    "to do:\n",
    "\n",
    "$$\\mathbf{\\hat y} = \\mathop{\\arg \\max}\\limits_{\\mathbf{y} \\in \\cal Y^N} P(\\mathbf{y})$$\n",
    "\n",
    "What will we get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The same $N$-tag long sequence for any sentence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Hidden Markov model</h3>\n",
    "\n",
    "<a href=\"http://www.slideshare.net/priberam/introducing-priberam-labs-machine-learning-and-natural-language-processing\"><img width=\"500\" src=\"images/hmm_pos_crop.png\"></a>\n",
    "\n",
    "PoS tags are hidden states emitting words. Assumptions:\n",
    "- 1st order Markov among the PoS tags\n",
    "- Each word only depends on its PoS tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Derivation\n",
    "\n",
    "$\\mathbf{\\hat y} = \\mathop{\\arg \\max}\\limits_{\\mathbf{y} \\in \\cal Y^N} P(\\mathbf{y}|\\mathbf{x})\\quad$ (Bayes rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\require{cancel} \\mathbf{\\hat y} = \\mathop{\\arg \\max}\\limits_{\\mathbf{y} \\in \\cal Y^N} \\frac{P(\\mathbf{x}|\\mathbf{y})P(\\mathbf{y})}{{\\cancel{P(\\mathbf{x}}})} \\quad$ (words are constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\mathbf{\\hat y} = \\mathop{\\arg \\max}\\limits_{\\mathbf{y} \\in \\cal Y^N} P(\\mathbf{x}|\\mathbf{y})P(\\mathbf{y}) \\quad$ (1st order Markov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\mathbf{\\hat y} \\approx \\mathop{\\arg \\max}\\limits_{\\mathbf{y} \\in \\cal Y^N} \\prod_{n=1}^NP(x_n|y_n)P(y_n|y_{n-1}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMM training\n",
    "\t\t\t\t\n",
    "Maximum likelihood estimation (i.e. counts!):\n",
    "\t\t\t\t\n",
    "$ P(y_n|y_{n-1}) = \\frac{counts(y_n,y_{n-1})}{counts(y_{n-1})} \\quad$  (transition probabilities)\n",
    "\t\t\t\t\n",
    "$ P(x_n|y_n) = \\frac{counts(x_n,y_n)}{counts(y_n)} \\quad$  (emission probabilities)\n",
    "\t\t\t\t\n",
    "We can easily read them off a labeled corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### In action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = ['I','can','fly']\n",
    "y = ['PPSS','MD','NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37161280973771915\n",
      "0.13876598825516853\n",
      "9.182134190332525e-05\n"
     ]
    }
   ],
   "source": [
    "emission_product = 1.0\n",
    "for i in range(len(x)):\n",
    "    prob = word_tag_counts[(y[i],x[i])]/unigram_tag_counts[y[i]] \n",
    "    emission_product *= prob\n",
    "    print(prob)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 'PPSS'), ('PPSS', 'MD'), ('MD', 'NN'), ('NN', None)]\n",
      "(None, 'PPSS'):0.05486571328915242\n",
      "('PPSS', 'MD'):0.15229676858426314\n",
      "('MD', 'NN'):0.0017697691255731639\n",
      "('NN', None):0.00044598937495900833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.122843277930901e-14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_LM(y)*emission_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decoding/Inference\n",
    "\n",
    "So we have everything we need to decode/infer the most likely tag sequence for a sentence:</p>\n",
    "\n",
    "$\\mathbf{\\hat y} = \\mathop{\\arg \\max}\\limits_{\\mathbf{y} \\in \\cal Y^N} \\prod_{n=1}^NP(x_n|y_n)P(y_n|y_{n-1}) \\quad$\n",
    "\n",
    "Just enumerate all possible tag sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "No! We would need to evaluate $|\\cal Y|^{N}$ sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Viterbi \n",
    "\t\t\t\t\n",
    "<a href=\"http://www.cse.unsw.edu.au/~billw/cs9414/notes/nlp/ambiguity/ambiguity-2012.html\"><img height=\"400\" src=\"images/viterbi.gif\"></a>\n",
    "\n",
    "- dynamic programming: store and re-use calculations\n",
    "- possible due to independence assumptions\n",
    "- keep track of the highest probability to reach each PoS tag for each word and how we got there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Viterbi data structures\n",
    "\n",
    "tag set $\\cal Y$, sentence $\\mathbf{x}=[x_1,... x_N]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Viterbi score matrix $V^{|{\\cal Y}|\\times N}$\n",
    "- each cell contains the highest prob. for word $n$ with tag $y$\n",
    "- 1st order Markov: only depends on the  previous tag $y^{\\prime}$\n",
    "- i.e. $V[y,n] = \\max_{y^{\\prime}\\in \\cal Y} V[y^{\\prime}, n-1] \\times P(y|y^{\\prime}) \\times P(x_n|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Backpointer matrix $backptr^{|{\\cal Y}|\\times N}$:\n",
    "- instead of the max score, keep the previous tag that got it\n",
    "- $argmax$ instead of $max$\n",
    "- i.e.: $backptr[y,n] = \\mathop{\\arg\\max}_{y^{\\prime}\\in \\cal Y} V[y^{\\prime}, n-1] \\times P(y|y^{\\prime}) \\times P(x_n|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Viterbi diagram\n",
    "\n",
    "![](images/jurafsky_5_18_viterbi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Viterbi algorithm</h3>\n",
    "<p style=\"border:3px; width: 1100px; border-radius: 25px; background-color:lightgrey; border-style:solid; border-color:black; padding: 0.3em;\">\n",
    "\\begin{align}\n",
    "& \\textbf{Input:} \\; word\\; sequence\\; \\mathbf{x}=[x_1,...,x_N],\\\\\n",
    "& emission \\; probs \\; P(x|y), transition \\; probs \\; P(y_n|y_{n-1})\\\\\n",
    "& set\\; matrix \\; V^{|{\\cal Y}|\\times N} = 1\\\\\n",
    "& \\mathbf{for} \\; n = 1 \\; \\mathbf{to} \\; N \\; \\mathbf{do}\\\\\n",
    "& \\quad \\mathbf{for} \\; y \\in \\cal Y \\; \\mathbf{do}\\\\\n",
    "& \\quad \\quad V[y,n] = \\max_{y^{\\prime}\\in \\cal Y} V[y^{\\prime}, n-1] \\times P(y|y^{\\prime}) \\times P(x_n|y)\\\\\n",
    "& \\quad \\quad backptr[y,n] = \\mathop{\\arg \\max}\\limits_{y^{\\prime}\\in \\cal Y} V[y^{\\prime}, n-1] \\times P(y|y^{\\prime})\\times P(x_n|y)\\\\\n",
    "& backptr[None,N+1] = \\mathop{\\arg \\max}\\limits_{y^{\\prime}\\in \\cal Y} V[y^{\\prime}, N] \\times P(None|y^{\\prime})\\\\\n",
    "\\end{align}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Break the large $\\arg\\max$ into smaller ones, left-to-right (**dynamic programming**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some more points\n",
    "\n",
    "Higher order HMMs:\n",
    "- longer contexts, more expensive inference\n",
    "- benefits are usually small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Smoothing:\n",
    "- what happens when we have unseen word/tags or tag-tag combinations?\n",
    "- everything we learned in the language modeling lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bibliography\n",
    "\n",
    "- Michael Collins's [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf)\n",
    "- J&M [chapter 9](https://web.stanford.edu/~jurafsky/slp3/9.pdf) from the new edition\n",
    "- Graham Neubig's [slides](http://www.phontron.com/slides/nlp-programming-en-04-hmm.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up next\n",
    "\n",
    "The perceptron and text classification!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
