{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine(a,b):\n",
    "    return np.dot(a,b)/(np.sqrt(np.dot(a,a))* np.sqrt(np.dot(b,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Distributed word representations</h2>\n",
    "<p style=\"text-align:center\">\n",
    "Natural Language Processing<br>\n",
    "(COM4513/6513)<br>\n",
    "<br>\n",
    "<a href=\"http://andreasvlachos.github.io\">Andreas Vlachos</a><br>\n",
    "a.vlachos@sheffield.ac.uk<br>\n",
    "<small>Department of Computer Science<br>\n",
    "University of Sheffield\n",
    "</small>\n",
    "</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So far\n",
    "\n",
    "We have represented words as discrete objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "represention_appricot = np.array([0,0,0,0,1,0,0,0])\n",
    "represention_pineapple = np.array([0,0,0,1,0,0,0,0])\n",
    "BoW_appricot_pineapple = represention_appricot +  represention_pineapple\n",
    "BoW_appricot_pineapple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also known as **one-hot encoding**. What's the problem with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Every word is equally different from every other word. But pineapple and apricot are related!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In this lecture\n",
    "\n",
    "We will challenge the discreteness of word representations.\n",
    "\n",
    "Instead of one-hot encodings, we will map words to distributed representations, a.k.a. **embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A quick test\n",
    "\n",
    "What is *tesguino*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some sentences mentioning it: \n",
    "- A bottle of *tesguino* is on the table.\n",
    "- Everybody likes *tesguino*\n",
    "- Tesguino makes you drunk.\n",
    "- We make tesguino out of corn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> You shall know a word by the company it keeps (Firth, 1957)\n",
    "\n",
    "Words appearing in similar contexts are likely have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word-context matrix\n",
    "\n",
    "![coocs](images/coocs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- typically **very sparse**\n",
    "- the shorter the window the more **syntactic** they are\n",
    "- the longer the window the more **semantic** they are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Representation above **syntactic** or **semantic**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-hot vs  distributed representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "appricot_1hot = np.array([1,0,0,0]) # length equal to vocabulary size\n",
    "pineapple_1hot = np.array([0,1,0,0])\n",
    "digital_1hot = np.array([0,0,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "appricot_dist = np.array([0,0,0,1,0,1]) # length equal to number of contexts from the table in the previous slide\n",
    "pineapple_dist = np.array([0,0,0,1,0,1])\n",
    "digital_dist = np.array([0,2,1,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How to calculate vector similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Similarity\n",
    "\n",
    "$\\text{dot-product}(\\mathbf{x_1},\\mathbf{x_2})= \\mathbf{x_1} \\cdot \\mathbf{x_2} = \\sum_{c=1}^{|C|} x_1^c x_2^c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(appricot_1hot, pineapple_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(appricot_dist, pineapple_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(appricot_dist, digital_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Works! Any problems with the dot product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Frequent words have more contexts, will be more similar to anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_dist = np.array([5,2,3,4,5,1])\n",
    "np.dot(the_dist, pineapple_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Solution: Divide by their length (a.k.a. cosine)\n",
    "\n",
    "$\\text{cosine}(\\mathbf{x_1}, \\mathbf{x_2}) =\n",
    "\t\t\t\t\t\t\t \\frac{\\mathbf{x_1} \\cdot \\mathbf{x_2}}{|\\mathbf{x_1}||\\mathbf{x_2}|} =\n",
    "\t\t\t\t\t\t\t \\frac{\\sum_{c=1}^{|C|}x_1^c x_2^c}{\\sqrt{\\sum_{c=1}^{|C|} (x_1^c)^2}\\sqrt{\\sum_{c=1}^{|C|} (x_2^c)^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39528470752104738"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(appricot_dist, the_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999978"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(appricot_dist, pineapple_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Other options: Dice, Jaccard, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem with counts\n",
    "\n",
    "Counts are OK, but frequent words (articles, pronouns, etc.) dominate contexts without being informative.\n",
    "\n",
    "Let's add *the* to the contexts which appears often with most nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97020845119663623"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appricot_dist_the = np.array([0,0,0,1,0,1,10]) # last element is the counts of \"the\"\n",
    "pineapple_dist_the = np.array([0,0,0,1,0,1,30])\n",
    "digital_dist_the = np.array([0,2,1,0,1,0,10])\n",
    "information_dist_the = np.array([0,1,6,0,4,0,20])\n",
    "cosine(digital_dist_the, pineapple_dist_the)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96746180284073735"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(digital_dist_the, information_dist_the)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pointwise mutual information\n",
    "\n",
    "Pointwise mutual information measures how often two events occur relative to them occurring independently:\n",
    "\n",
    "$$PMI(word,context) = \\log_2 \\frac{P(word,context)}{P(word)P(context)}$$\n",
    "\n",
    "Positive values quantify relatedness.  Use PMI instead of counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Negative values? Usually ignored (positive PMI):\n",
    "\n",
    "$$PPMI(word,ctxt) = \\max(PMI(word,ctxt),0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choice of contexts\n",
    "\n",
    "We can refine contexts using:\n",
    "- their part-of-speech tags (*bank_V* vs. *bank_N*)\n",
    "- syntactic dependencies (*eat_dobj* vs. *eat_subj*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can weigh contexts according to the distance from the word: the further away, the lower the weight.\n",
    "\n",
    "If we are using a window size $w$, multiply the context word at each position as $\\frac{w-distance}{w}$, e.g. for $w=3$:\n",
    "\n",
    "$[\\frac{1}{3}, \\frac{2}{3}, \\frac{3}{3}, word, \\frac{3}{3}, \\frac{2}{3}, \\frac{1}{3}]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular value decomposition\n",
    "\n",
    "PPMI matrices are good, but:\n",
    "- high dimensional\n",
    "- very sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dimensionality reduction using *truncated* singular value decomposition:\n",
    "\n",
    "$$ PPMI^{|V|\\times|C|} \\approx W^{|V|\\times k}  S^{k \\times k} C^{k \\times |C|} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Approximation is good: exploits redundancy to remove noise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Singular value decomposition\n",
    "\n",
    "<img src=\"images/trunc_svd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-gram (Mikolov et al. 2013)\n",
    "\n",
    "Running SVD on large matrices is expensive.\n",
    "\n",
    "Let's look at one word a time:\n",
    "\n",
    "$P(ctxt|w_t) = P(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}|w_t)  \\quad$ (<b>skip</b>-gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(ctxt|w_t) = \\prod_{w_c \\in ctxt} P(w_c | w_t)  \\quad \\quad\\quad$ (word independence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$P(w_c | w_t) = \\frac{\\exp(\\mathbf{w_c} \\cdot \\mathbf{w_t})}{\\sum_{w_c^\\prime \\in V} \\exp(\\mathbf{w_c}^\\prime \\cdot \\mathbf{w_t}) } \\quad \\quad$ (each word $w$ is a word vector $\\mathbf{w}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A **giant logistic regression classifier**: words are the labels. Raw text gives us positive examples, negative ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Negative sampling**: negative training examples are sub-sampled randomly, all positive ones are kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Skip-gram\n",
    "\n",
    "<img src=\"images/skipgram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Each word has two embeddings\n",
    "![embedding](images/embedding.png)\n",
    "\n",
    "Can discard the context word embeddings, add them, or concatenate them with the target word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Intrinsic:\n",
    "- similarity: order word pairs according to their semantic similarity\n",
    "- in-context similarity: substitute a word in a sentence without chagning its meaning.\n",
    "- analogy: Athens is to Greece what Rome is to ...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Extrinsic: use them to improve performance in a task: instead of bag of words, **bag of word vectors** \n",
    "\n",
    "They are an easy way to take advantage of unlabeled data to do **semi-supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Best word vectors?\n",
    "\n",
    "- high-dimensional (processed) counts?\n",
    "- low-dimensional neural/SVD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recent paper by Levy et al. (2015) showed that choice of context window size, rare word removal, etc. matter more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Choice of texts to obtain the counts matters. More text is better, and low-dimensional methods scale better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about polysemy?\n",
    "\n",
    "All occurrences of a word (and all its senses) are represented by one vector.\n",
    "\n",
    "How do we handle polysemy?\n",
    "- all senses are present in the vector\n",
    "- given a task, it is often useful to adapt the vectors to represent the appropriate sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations\n",
    "\n",
    "- **antonyms appear** in similar contexts, hard to distinguish them from synonyms\n",
    "\n",
    "- **compositionality**: what is the meaning of a sequence of words? while we might be able to obtain context vectors for short phrases, this doesn't scale to whole sentences, paragraphs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Soon we will see methods dedicated to learning embeddings for word sequences from word embeddings, the recurrent neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bibliography\n",
    "- Jurafsky & Martin [Chapter 15](https://web.stanford.edu/~jurafsky/slp3/15.pdf) and [Chapter 16](https://web.stanford.edu/~jurafsky/slp3/16.pdf)\n",
    "- Omer Levy's [article](https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf)\n",
    "- Turian et al.'s [paper](http://www.aclweb.org/anthology/P10-1040) on how to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up next\n",
    "\n",
    "Neural Networks"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
