{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h2>Recurrent Neural Networks</h2>\n",
    "<p style=\"text-align:center\">\n",
    "Natural Language Processing<br>\n",
    "(COM4513/6513)<br>\n",
    "<br>\n",
    "<a href=\"http://andreasvlachos.github.io\">Andreas Vlachos</a><br>\n",
    "a.vlachos@sheffield.ac.uk<br>\n",
    "<small>Department of Computer Science<br>\n",
    "University of Sheffield\n",
    "</small>\n",
    "</p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some reminders\n",
    "\n",
    "N-gram language models:\n",
    "- large-scale classifiers predicting the next word\n",
    "- sparsity issues: smoothing, back-off, interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Neural networks:\n",
    "- can learn more complex decision boundaries\n",
    "- can learn feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Continuous word representations: words are no longer discrete objects,\n",
    "but vectors that capture semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In this lecture\n",
    "\n",
    "Recurrent Neural networks applied to language modelling\n",
    "- revolutionized the way a lot of people think in NLP\n",
    "- return word probabilities conditioned on all previous words as well as **sentence representations**\n",
    "- their popularity can't be over-stated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem setup\n",
    "\n",
    "Training data is a (large) set of sentences $\\mathbf{x}^m$ with words $x_n$:\n",
    "\n",
    "<p>\n",
    "\\begin{align}\n",
    "D_{train} & = \\{\\mathbf{x}^1,...,\\mathbf{x}^M\\} \\\\\n",
    "\\mathbf{x}& = [x_1,... x_N]\\\\\n",
    "\\end{align}\n",
    "</p>\n",
    "\n",
    "<p class=\"fragment\">\n",
    "for example:\n",
    "\\begin{align}\n",
    "\\mathbf{x}=&[\\text{None}, \\text{The}, \\text{water}, \\text{is}, \\text{clear}, \\text{.}, \\text{None}]\n",
    "\\end{align}\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We want to learn a model that returns:\n",
    "\\begin{align}\n",
    "\\text{probability}\\; P(\\mathbf{x}), \\mathbf{for} \\; \\forall \\mathbf{x}\\in V^{maxN}\n",
    "\\end{align}\n",
    "$V$ is the vocabulary and $V^{maxN}$ all possible sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rethinking Language Modelling\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathbf{x}) &= P(x_1,...,x_N) \\\\\n",
    "&= P(x_1)P(x_2...x_N|x_1)\\\\\n",
    "&= P(x_1)P(x_2|x_1) ... P(x_N|x_1,...,x_{N-1})\\\\\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "$$P(x_n = k| x_{n-1...x_1})=\\frac{counts(x_1...x_{n-1}, k)}{counts(x_1...x_{n-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A logistic regression classifier predicting the next word:\n",
    "\n",
    "\\begin{align}\n",
    "p(x_n = k| x_{n-1}... x_1) &= \\frac{\\exp(\\mathbf{w}_k \\cdot \\phi(x_{n-1}... x_1) )}{\\sum_{k^\\prime=1}^{|\\cal V|}\\exp(\\mathbf{w}_{k^\\prime} \\cdot \\phi(x_{n-1}... x_1) )} \\\\\n",
    "&= softmax(\\mathbf{W}\\cdot\\phi(x_{n-1}... x_1))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why this woudn't work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sparsity\n",
    "\n",
    "\\begin{align}\n",
    "p(x_n = k| x_{n-1}... x_1) &= \\frac{\\exp(\\mathbf{w}_k \\cdot \\phi(x_{n-1}... x_1) )}{\\sum_{k^\\prime=1}^{|\\cal V|}\\exp(\\mathbf{w}_{k^\\prime} \\cdot \\phi(x_{n-1}... x_1) )} \\\\\n",
    "&= softmax(\\mathbf{W}\\cdot\\phi(x_{n-1}... x_1))\n",
    "\\end{align}\n",
    "\n",
    "- $\\mathbf{w}_k$ are the weights for word $k$</li>\n",
    "- $\\phi(x_{n-1}... x_1)$ are the features extracted from the previous words (one-hot encoding of $x_{n-1}... x_1$)\n",
    "- dimensionality of $\\mathbf{W}$ is $|V|\\times |V|^n$ (vocabulary size, contexts)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We avoided the issue using the Markov assumption (**N-gram** LMs), but could we do something different without restricting the contexts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Skip gram reminder\n",
    "\n",
    "<img src=\"images/skipgram.png\"  style=\"width:650px;\">\n",
    "\n",
    "$P(w_{t-1} | w_t) = \\frac{\\exp(\\mathbf{c_{t-1}} \\cdot \\mathbf{w_t})}{\\sum_{c^\\prime \\in V} \\exp(\\mathbf{c}^\\prime \\cdot \\mathbf{w_t}) }$\n",
    "\n",
    "Each word is has two vectors, one for itself ($\\mathbf{w}$) and one as context ($\\mathbf{c}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From word embeddings to word sequence embeddings\n",
    "\n",
    "\\begin{align}\n",
    "p(x_n = k| x_{n-1}... x_1) &= \\frac{\\exp(\\mathbf{w}_k \\cdot \\phi(x_{n-1}... x_1) )}{\\sum_{k^\\prime=1}^{|\\cal V|}\\exp(\\mathbf{w}_{k^\\prime} \\cdot \\phi(x_{n-1}... x_1) )} \\\\\n",
    "&= softmax(\\mathbf{W}\\cdot\\phi(x_{n-1}... x_1))\n",
    "\\end{align}\n",
    "\n",
    "Let's assume we have word embeddings $\\mathbf{W}\\in \\Re^{|\\cal V|\\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to have a $\\phi(x_{n-1}... x_1)$ that gives us a continuous representation in $\\Re^{d\\times 1}$ for each context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent neural networks\n",
    "\n",
    "<a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\"><img src=\"images/rnn.jpg\" style=\"width:700px; background:none; border:none; box-shadow:none;\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- **input**: $x_t \\in \\{0,1\\}^{|\\cal V|}$ are the words in one-hot encoding\n",
    "- **hidden**: $s_{t-1} \\in \\Re^d$: \"memory\" of the context until word $x_{t-1}$\n",
    "- **output**: $\\mathbf{o}_{t-1} = p(x_{t}| x_{t-1}... t_1) \\in \\Re^{|\\cal V|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recurrent neural networks\n",
    "\n",
    "<a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\"><img src=\"images/rnn.jpg\" style=\"width:700px; background:none; border:none; box-shadow:none;\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Parameters to be learned:\n",
    "- $\\mathbf{U}\\in \\Re^{d\\times |\\cal V|}$: matrix containing the word vectors for all the words, $x_n$ picks one\n",
    "- $\\mathbf{W}\\in \\Re^{d \\times d}$: controls how this memory is passed on\n",
    "- $\\mathbf{V}\\in \\Re^{|\\cal V| \\times d}$: maps the memory to probability for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recurrent neural networks\n",
    "\n",
    "<a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\"><img src=\"images/rnn.jpg\" style=\"width:700px; background:none; border:none; box-shadow:none;\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$s_t = \\sigma(\\mathbf{W}s_{t-1} + \\mathbf{U}x_t)$\n",
    "\n",
    "$\\mathbf{o}_{t} = p(x_{n+1}| x_{t}... x_1) = softmax(\\mathbf{V}s_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training RNNs\n",
    "\n",
    "We need to learn the word vectors $\\mathbf{U}$, hidden and output layer parameters $\\mathbf{W}, \\mathbf{V}$\n",
    "\n",
    "Standard backpropagation can't work because of the recurrence:  we reuse the hidden layer parameters $\\mathbf{W}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Backpropagation through time**: unroll the graph for $n$ steps and sum the gradients in updating\n",
    "\n",
    "Not as restrictive as the $n$th-order Markov: we still use all previous words through the recurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long-range dependencies\n",
    "\n",
    "RNNs try but can't capture long-range dependencies:\n",
    "- effectively ave one layer per word in the sentence\n",
    "- all context information has to be passed by the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a href=\"https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/\"><img src=\"images/LSTM.png\" style=\"width:600px; background:none; border:none; box-shadow:none;\" /></a>\n",
    "\n",
    "**Long-short term memory networks** address this by adding an extra \"memory\" cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variants\n",
    "\n",
    "In language modelling we use word sequence representations at each time-step to predict the next word. Other tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\"><img src=\"images/rnns.jpeg\" width=\"800\" style=\"background:none; border:none; box-shadow:none;\" /></a>\n",
    "- many to one (sentiment analyis)\n",
    "- many to many (equal) (e.g. PoS tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representations\n",
    "\n",
    "RNNs learn word and sentence representations\n",
    "\n",
    "Words are not as interesting since RNNs are slower to train than Skip-Gram:\n",
    "thus use less data\n",
    "- hint: use skipgram to initialize (pre-train) the RNN word vectors\n",
    "\n",
    "RNN sentence representations though are used often!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Textual entailment</h3>\n",
    "<a href=\"http://arxiv.org/abs/1509.06664\"><img src=\"images/rockt.jpg\" style=\"width:800px; background:none; border:none; box-shadow:none;\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Machine translation</h3>\n",
    "\n",
    "<a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\"><img src=\"images/rnn_mt.png\" style=\"width:800px; background:none; border:none; box-shadow:none;\" /></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional neural nets:\n",
    "- operate on short context windows, e.g. 5 words\n",
    "- popular in vision to model pixel neighborhoods\n",
    "- used in  tasks where modelling the full sequence (sentence, document) is not more useful than modelling parts.\n",
    "\n",
    "<a href=\"http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\"><img src=\"images/CNNs_NLP.png\" width=\"1000\" style=\"background:none; border:none; box-shadow:none;\" /></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multimodal processing\n",
    "\n",
    "<a href=\"http://kelvinxu.github.io/projects/capgen.html\"><img src=\"images/capgen.png\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bibliography\n",
    "\n",
    "- The lecture followed roughly Cho's [lecture notes](http://arxiv.org/pdf/1511.07916v1.pdf) (section 5.5, but check references to earlier chapters)\n",
    "- [Notes](https://web.stanford.edu/class/cs224n/lecture_notes/cs224n-2017-notes5.pdf) from Stanford's NLP course\n",
    "- This [blog](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) has great visualizations, easy to read code\n",
    "- For more on LSTMs see this [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) from where their figures were taken and [this](http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/) for some python code\n",
    "- For more NLP references check Yoav Goldberg's [tutorial](http://u.cs.biu.ac.il/~yogo/nnlp.pdf), section 10\n",
    "- Chapter 6, 7 and 8 from the just released [deep learning book](http://www.deeplearningbook.org), chapter 10 and section 12.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coming up next\n",
    "\n",
    "Natural Language Generation\n",
    "\n",
    "Machine Translation"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "start_slideshow_at": "selected",
   "theme": "solarized",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
